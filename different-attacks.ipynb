{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#os.chdir('..')\n",
    "\n",
    "from attacks import influence_attack, anchoring_attack\n",
    "from datamodules import GermanCreditDatamodule, CompasDatamodule, DrugConsumptionDatamodule\n",
    "from fairness import FairnessLoss\n",
    "from trainingmodule import BinaryClassifier\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a **general attack function** that handles all different attack methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(dm, model, eps, method):\n",
    "    if method == 'IAF':\n",
    "        # Create adversarial loss according to Mehrabi et al.\n",
    "        bce_loss, fairness_loss = BCEWithLogitsLoss(), FairnessLoss(dm.get_sensitive_index())\n",
    "        adv_loss = lambda _model, X, y: (\n",
    "                bce_loss(_model(X), y.float()) + 0.1 * fairness_loss(X, *_model.get_params())\n",
    "        )\n",
    "        \n",
    "        # Create new training pipeline to use in influence attack\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            gpus=1 if torch.cuda.is_available() else 0,\n",
    "            enable_model_summary=False,\n",
    "            enable_progress_bar=False,\n",
    "            log_every_n_steps=1,\n",
    "            callbacks=[EarlyStopping(monitor=\"train_loss\", mode=\"min\", patience=10)]\n",
    "        )\n",
    "\n",
    "        poisoned_dataset = influence_attack(\n",
    "            model=model,\n",
    "            datamodule=dm,\n",
    "            trainer=trainer,\n",
    "            adv_loss=adv_loss,\n",
    "            eps=eps,\n",
    "            eta=0.01,\n",
    "            attack_iters=100,\n",
    "        )\n",
    "    elif method in ['RAA', 'NRAA']:\n",
    "        poisoned_dataset = anchoring_attack(\n",
    "            D_c=dm.get_train_dataset(),\n",
    "            sensitive_idx=dm.get_sensitive_index(),\n",
    "            eps=eps,\n",
    "            tau=0,\n",
    "            sampling_method='random' if attack == 'RAA' else 'non-random',\n",
    "            attack_iters=1,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown attack {method}.')\n",
    "    \n",
    "    # Create deep copy of the original dataset and poison the copy\n",
    "    dm = deepcopy(dm)\n",
    "    dm.update_train_dataset(poisoned_dataset)\n",
    "\n",
    "    return dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a **nested dictionary**, which is convinient to store results for multiple datasets and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_dict():\n",
    "   return collections.defaultdict(nested_dict)\n",
    "\n",
    "results = nested_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, iterate over all possible combination of Figure 2 in Mehrabi et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisoning German Credit dataset with IAF attack:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eed7401f5424bdc9f2cf1e4f29fb9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'EOD': 0.23472219705581665,\n",
      " 'SPD': 0.1278996765613556,\n",
      " 'test_error': 0.29500001668930054}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/cvxpy/reductions/solvers/solving_chain.py:167: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n",
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/john/Desktop/MSc AI/FACT-AI/lightning_logs/version_18/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/john/Desktop/MSc AI/FACT-AI/lightning_logs/version_17/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'EOD': 0.01527780294418335,\n",
      " 'SPD': 0.09717869758605957,\n",
      " 'test_error': 0.4300000071525574}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/john/Desktop/MSc AI/FACT-AI/lightning_logs/version_19/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'EOD': 0.09999999403953552,\n",
      " 'SPD': 0.14294672012329102,\n",
      " 'test_error': 0.5850000381469727}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/john/Desktop/MSc AI/FACT-AI/lightning_logs/version_20/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'EOD': 0.05972221493721008,\n",
      " 'SPD': 0.10595610737800598,\n",
      " 'test_error': 0.5600000023841858}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/john/Desktop/MSc AI/FACT-AI/lightning_logs/version_21/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'EOD': 0.15833333134651184,\n",
      " 'SPD': 0.05830717086791992,\n",
      " 'test_error': 0.47999998927116394}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/john/Desktop/MSc AI/FACT-AI/lightning_logs/version_22/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'EOD': 0.26944443583488464,\n",
      " 'SPD': 0.2796238362789154,\n",
      " 'test_error': 0.5149999856948853}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/john/Desktop/MSc AI/FACT-AI/lightning_logs/version_23/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/john/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m BinaryClassifier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m, dm\u001b[38;5;241m.\u001b[39mget_input_size(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create poisoned dataset\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m poisoned_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train on the poisoned dataset\u001b[39;00m\n\u001b[1;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, dm)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mattack\u001b[0;34m(dm, model, eps, method)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Create new training pipeline to use in influence attack\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     11\u001b[0m         max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m     12\u001b[0m         gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m     poisoned_dataset \u001b[38;5;241m=\u001b[39m \u001b[43minfluence_attack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43madv_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madv_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattack_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNRAA\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     29\u001b[0m     poisoned_dataset \u001b[38;5;241m=\u001b[39m anchoring_attack(\n\u001b[1;32m     30\u001b[0m         D_c\u001b[38;5;241m=\u001b[39mdm\u001b[38;5;241m.\u001b[39mget_train_dataset(),\n\u001b[1;32m     31\u001b[0m         sensitive_idx\u001b[38;5;241m=\u001b[39mdm\u001b[38;5;241m.\u001b[39mget_sensitive_index(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         attack_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/MSc AI/FACT-AI/attacks/influence.py:61\u001b[0m, in \u001b[0;36minfluence_attack\u001b[0;34m(model, datamodule, trainer, adv_loss, eps, eta, attack_iters, project_fn, defense_fn, get_defense_params, get_minimization_problem)\u001b[0m\n\u001b[1;32m     58\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloader)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Precompute g_θ (H inverse is too expensive for analytical computation)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m g_theta \u001b[38;5;241m=\u001b[39m \u001b[43m__compute_g_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m minimization_problem \u001b[38;5;241m=\u001b[39m get_minimization_problem(ConcatDataset([D_c, D_p]))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "File \u001b[0;32m~/Desktop/MSc AI/FACT-AI/attacks/influence.py:95\u001b[0m, in \u001b[0;36m__compute_g_theta\u001b[0;34m(model, dataset, loss)\u001b[0m\n\u001b[1;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# zero gradients for safety\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Accumulate model's gradients over dataset\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m L\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mget_grads()\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mattack.<locals>.<lambda>\u001b[0;34m(_model, X, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIAF\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Create adversarial loss according to Mehrabi et al.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     bce_loss, fairness_loss \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss(), FairnessLoss(dm\u001b[38;5;241m.\u001b[39mget_sensitive_index())\n\u001b[1;32m      5\u001b[0m     adv_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m _model, X, y: (\n\u001b[0;32m----> 6\u001b[0m             bce_loss(\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mfloat()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m fairness_loss(X, \u001b[38;5;241m*\u001b[39m_model\u001b[38;5;241m.\u001b[39mget_params())\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Create new training pipeline to use in influence attack\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     11\u001b[0m         max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m     12\u001b[0m         gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m     17\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/MSc AI/FACT-AI/trainingmodule.py:35\u001b[0m, in \u001b[0;36mBinaryClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/MSc AI/FACT-AI/models/logistric_regression.py:13\u001b[0m, in \u001b[0;36mLogisticRegression.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchEnv/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "# Create Datamodules for all datasets\n",
    "german_credit_datamodule = GermanCreditDatamodule('data/', 200)\n",
    "compas_datamodule = CompasDatamodule('data/', 200)\n",
    "drug_consumption_datamodule = DrugConsumptionDatamodule('data/', 200)\n",
    "\n",
    "# Create Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    enable_model_summary=False,\n",
    "    enable_progress_bar=False,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[EarlyStopping(monitor=\"train_loss\", mode=\"min\", patience=20)]\n",
    ")\n",
    "\n",
    "for dm in [german_credit_datamodule, compas_datamodule, drug_consumption_datamodule]:\n",
    "    for method in ['IAF', 'RAA', 'NRAA']:\n",
    "        print(f'Poisoning {dm.get_dataset_name()} dataset with {method} attack:')\n",
    "        for eps in tqdm(np.arange(0, 1.1, 0.1)):\n",
    "            # Create a Binary Classifier model for each dataset\n",
    "            model = BinaryClassifier('LogisticRegression', dm.get_input_size(), lr=1e-3)\n",
    "            \n",
    "            # Create poisoned dataset\n",
    "            poisoned_dataset = attack(dm, model, eps, method)\n",
    "            \n",
    "            # Train on the poisoned dataset\n",
    "            trainer.fit(model, poisoned_dataset)\n",
    "            \n",
    "            # Save Accuracy and Fairness metrics\n",
    "            metrics = trainer.test(model, dm)[0]\n",
    "            results[dm.get_dataset_name()]['Test Error'][method][eps] = metrics['test_error']\n",
    "            results[dm.get_dataset_name()]['Statistical Parity'][method][eps] = metrics['SPD']\n",
    "            results[dm.get_dataset_name()]['Equality of Opportunity'][method][eps] = metrics['EOD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce plot styling of the original paper\n",
    "colors, markers = ['b', 'r', 'g'], ['s', 'x', '^']\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(20, 10))\n",
    "for i, dataset in enumerate(['German Credit', 'COMPAS', 'Drug Consumption']):\n",
    "    for j, metric in enumerate(['Test Error', 'Statistical Parity', 'Equality of Opportunity']):\n",
    "        for k, method in enumerate(['IAF', 'RAA', 'NRAA']):\n",
    "            ax[i, j].plot(\n",
    "                list(results[dataset][metric][method].keys()),\n",
    "                list(results[dataset][metric][method].values()),\n",
    "                c=colors[k],\n",
    "                marker=markers[k],\n",
    "                label = method,\n",
    "            )\n",
    "        \n",
    "        ax[i, j].set_xlabel('$\\epsilon$', fontweight='bold')\n",
    "        ax[i, j].set_ylabel(metric, fontweight='bold')\n",
    "        ax[i, j].set_title(dataset, fontweight='bold')\n",
    "        ax[i, j].legend(loc='upper left', ncol=3)\n",
    "        \n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c11462fc89fac378f146398dd73443d105d4de5c614974541e5ab141425dc19"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
